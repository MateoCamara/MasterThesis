# -*- coding: utf-8 -*-
"""dense_sequential.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jTZOTHpXcVlA5dRYInEimXvW3I1F4zWs

# IMPORTS
"""

!pip install keras_tqdm

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
import glob

os.environ['KERAS_BACKEND']='tensorflow'
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, TimeDistributed, LSTM, Reshape
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from keras.optimizers import Adam, SGD
from keras import backend as BK
from keras.models import Model
from sklearn.metrics import mean_squared_error
from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, Callback

from matplotlib import pyplot
from keras.preprocessing.image import img_to_array, load_img
from tqdm import tqdm_notebook as tqdm

import matplotlib.pyplot as plt
import seaborn as sns
from os import listdir
from os.path import isfile, join


from keras_tqdm import TQDMNotebookCallback

import threading
import random



# Any results you write to the current directory are saved as output.

"""# DEPENDENCIAS"""

working_on = 'gdrive' # OR gdrive

if working_on == 'local':
  print("alv")
else:
  data_path = "./gdrive/My Drive/TFM/videos/complexities_diffs_gray_nocomunes.csv"
  #data_path = "./gdrive/My Drive/TFM/videos/complexities_live.csv"
  from google.colab import drive
  drive.mount('/content/gdrive')

"""#  VARIABLES GLOBALES"""

## GLOBAL VARIABLES ## 
version=1
learning_rate = 1e-7
_epochs=1000
_batch_size=5
optimizer="adam"

"""#CARGAR LOS DATOS"""

## DATOS NUMÉRICOS
data = pd.read_csv(data_path,index_col=None, header=0)
df_shuf = data.sample(frac=1, random_state=1)
dataset = df_shuf.values
dataset.shape

dataset

"""#EXPLORATORY ANALYSIS"""

## A BIT MORE OF EDA (Rather than only R) ## 
# qué pinta tiene
data.head()

# Estadísticos varios
data.describe()

# Tipos de datos
data.info()

## VISUALIZACIÓN DE DATOS
plt.figure(figsize=(10,8))
plt.rcParams.update({'font.size': 22})

plt.hist(x='DMOS', bins=30,data=data)




plt.show()

plt.rcParams.update({'font.size': 22})

plt.figure(figsize=(10,8))
plt.style.use('seaborn-whitegrid')


plt.boxplot(data['DMOS'], showmeans=True, whis = 99)
plt.xticks([1], ['DMOS'])

data

diablo = data.drop(columns=['stdev', 'confidence'])

diablo = diablo.drop(columns=['low'])

diablo = diablo.rename(index=str, columns={"midlow": "low", "midhigh": "mid"})

plt.rcParams.update({'font.size': 10})

sns.heatmap(diablo.corr(),annot=True,cmap='RdYlGn',xticklabels=True, yticklabels=True)
fig=plt.gcf()
fig.set_dpi(100)
fig.set_size_inches(12,10)
plt.show()

## Limpieza de variables
del data, df_shuf

"""# SEPARAR ENTRE TRAIN Y TEST"""

dataset[:,0]

X = dataset[:,1:20]
Y = dataset[:,20]
confidence = dataset[:,22]

X.shape

#np.savetxt('./gdrive/My Drive/TFM/resultados/datos_utilizados.csv',dataset,delimiter=',', fmt='%s')

from sklearn import preprocessing

min_max_scaler = preprocessing.MinMaxScaler()
X = min_max_scaler.fit_transform(X)

PERCENTAGE_SPLIT_TRAIN = 0.9

length_split_train = X.shape[0]*PERCENTAGE_SPLIT_TRAIN
length_split_train = int(length_split_train)

xtrain=X[0:length_split_train]
xtest=X[length_split_train:X.shape[0]]
ytrain=Y[0:length_split_train]
ytest=Y[length_split_train:Y.shape[0]]
yconfidence=confidence[length_split_train:X.shape[0]]

xtest

"""# GENERACIÓN DE LA RED NEURONAL"""

def model():
    # los rangos de las variables del input son      229 65 25 16 15 86 59 24 40 17 11 3 1  
    # la primera aproximación que voy a hacer es precisamente esa
    
    model = Sequential()
    model.add(Dense(32, input_dim=19, kernel_initializer='normal', activation='relu'))   
    model.add(Dense(64, kernel_initializer='normal', activation='relu'))   
    #model.add(Dense(64, kernel_initializer='normal', activation='relu'))   
    model.add(Dense(128, kernel_initializer='normal', activation='relu'))   

    #model.add(Dense(256, kernel_initializer='normal', activation='relu'))   

    model.add(Dense(1, kernel_initializer='normal', activation='exponential'))
    
    # Compile model
    adam = Adam(lr=learning_rate)
    model.compile(loss='mean_squared_error', optimizer=optimizer)
    return model

"""# ENTRENAMIENTO DE LA RED"""

seed = 7
np.random.seed(seed)
# evaluate model with standardized dataset
estimator = KerasRegressor(build_fn=model, epochs=_epochs, batch_size=_batch_size, verbose=1)

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)

model().summary()

history = estimator.fit(xtrain, ytrain, validation_split=0.1, callbacks=[es])

## NO SE PUEDE USAR TENSORBOARD CON DATOS DE VALIDACION QUE SEAN GENERADORES
#tbCallBack = TensorBoard(log_dir='./Graph3dCNN', histogram_freq=1, write_graph=True, write_images=True)

prediction = estimator.predict(xtest)
mean_squared_error(ytest, prediction)

from scipy.stats.stats import pearsonr
pearsonr(ytest, prediction)

estimator.model.save('./gdrive/My Drive/TFM/notebooks/checkpoints_dense/dense_model.h5')

pyplot.figure(figsize=(20,10))
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='validation')
pyplot.legend()
pyplot.show()

## LO PREDICHO CONTRA LO REAL ##
plt.style.use('seaborn-whitegrid')
plt.figure(figsize=(20,9))
plt.rcParams.update({'font.size': 22})
plt.scatter(ytest.astype(float), prediction,  color='green', alpha=1)
plt.xlabel('real', fontsize=22)
plt.ylabel('predicho', fontsize=22)
x = np.linspace(0, 4, 5)
y = np.linspace(0, 4, 5)
plt.plot(x, y, color='red');


e = np.linspace(0, 4, 5)
f = np.linspace(1.5, 1.5, 5)
plt.plot(e, f,'--', color='yellow');

g = np.linspace(0, 4, 5)
h = np.linspace(2.5, 2.5, 5)
plt.plot(g, h,'--', color='yellow');

i = np.linspace(0, 4, 5)
j = np.linspace(3.5, 3.5, 5)
plt.plot(i, j,'--', color='yellow');

k = np.linspace(0, 4, 5)
l = np.linspace(4.5, 4.5, 5)
plt.plot(k, l,'--', color='yellow');



plt.errorbar(ytest.astype(float), prediction, yerr=yconfidence, fmt='.k');

plt.show()

correctos = 0
incorrectos = 0

for a in range(len(prediction)):
  if (yconfidence[a]+prediction[a]) >= ytest[a] and (prediction[a]-yconfidence[a]) <= ytest[a]:
    correctos = correctos+1
  else:
    incorrectos = incorrectos+1
    
print("correctos: " + str(correctos))
print("incorrectos: " + str(incorrectos))

"""# RANDOM FOREST"""

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_jobs=-1, verbose=1)
model.set_params(n_estimators=1000)
model.fit(xtrain, ytrain)

predict = model.predict(xtest)

mean_squared_error(ytest, predict)

from scipy.stats.stats import pearsonr
pearsonr(ytest, predict)

#index = 0
#ok_values = ytest

#for i in tqdm(ytest):
  
#  if abs(prediction[index]-ytest[index]) <=1:
#    ok_values[index] = 1
#  else:
#    ok_values[index]=0 
#  index = index+1

ok_values

## LO PREDICHO CONTRA LO REAL ##
plt.figure(figsize=(20,10))
plt.scatter(ytest, predict,  color='green', alpha=1)
plt.xlabel('real')
plt.ylabel('predicho')
plt.title('real contra predicho')
x = np.linspace(1, 5, 5)
y = np.linspace(1, 5, 5)
plt.plot(x, y, color='red');

a = np.linspace(1, 5, 5)
b = np.linspace(1.3, 5.3, 5)
plt.plot(a, b,'--', color='blue');

c = np.linspace(1, 5, 5)
d = np.linspace(0.7, 4.7, 5)
plt.plot(c, d,'--', color='blue');

e = np.linspace(1, 5, 5)
f = np.linspace(1.5, 1.5, 5)
plt.plot(e, f,'--', color='yellow');

g = np.linspace(1, 5, 5)
h = np.linspace(2.5, 2.5, 5)
plt.plot(g, h,'--', color='yellow');

i = np.linspace(1, 5, 5)
j = np.linspace(3.5, 3.5, 5)
plt.plot(i, j,'--', color='yellow');

k = np.linspace(1, 5, 5)
l = np.linspace(4.5, 4.5, 5)
plt.plot(k, l,'--', color='yellow');

plt.show()

"""# SVM"""

from sklearn.svm import SVR

svr_rbf = SVR(kernel='rbf', C=100, gamma=0.01, epsilon=.001, verbose=True)

y_rbf = svr_rbf.fit(xtrain, ytrain).predict(xtest)

mse = mean_squared_error(ytest, y_rbf)
print("El error cuadrático medio es: " + str(mse))

from scipy.stats.stats import pearsonr
corr_ = pearsonr(ytest, y_rbf)
print("La correlación entre valores reales y predichos es: " + str(corr_[0]))

## LO PREDICHO CONTRA LO REAL ##
plt.figure(figsize=(20,10))
plt.scatter(ytest, y_rbf,  color='green', alpha=1)
plt.xlabel('real')
plt.ylabel('predicho')
plt.title('real contra predicho (SVM)')
x = np.linspace(1, 5, 5)
y = np.linspace(1, 5, 5)
plt.plot(x, y, color='red');

a = np.linspace(1, 5, 5)
b = np.linspace(1.3, 5.3, 5)
plt.plot(a, b,'--', color='blue');

c = np.linspace(1, 5, 5)
d = np.linspace(0.7, 4.7, 5)
plt.plot(c, d,'--', color='blue');

e = np.linspace(1, 5, 5)
f = np.linspace(1.5, 1.5, 5)
plt.plot(e, f,'--', color='yellow');

g = np.linspace(1, 5, 5)
h = np.linspace(2.5, 2.5, 5)
plt.plot(g, h,'--', color='yellow');

i = np.linspace(1, 5, 5)
j = np.linspace(3.5, 3.5, 5)
plt.plot(i, j,'--', color='yellow');

k = np.linspace(1, 5, 5)
l = np.linspace(4.5, 4.5, 5)
plt.plot(k, l,'--', color='yellow');

plt.show()